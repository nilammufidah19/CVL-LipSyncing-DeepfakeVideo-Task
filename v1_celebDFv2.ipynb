{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VUf07AfNxTkD"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import (\n","    Dense, Reshape, Input, concatenate, Layer, Activation, Flatten,\n","    Dropout, BatchNormalization, Conv2D, MaxPooling2D, GlobalAveragePooling1D\n",")\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import BinaryCrossentropy\n","from skimage.metrics import structural_similarity as ssim\n","import numpy as np\n","from sklearn.metrics import average_precision_score\n","\n","from tensorflow.keras.layers import LayerNormalization"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSHj-txO1Dux","executionInfo":{"status":"ok","timestamp":1764137237819,"user_tz":-420,"elapsed":20901,"user":{"displayName":"Nilam Mufidah","userId":"08628876195916431623"}},"outputId":"d5e39e3e-a359-4801-ca2e-f93b0844c6c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# %cd \"/content/drive/MyDrive/MKA/semester 2/CVL/Tugas Kelompok/code\""],"metadata":{"id":"KYDstLef1E1L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import kagglehub\n","\n","# # Download latest version\n","# path = kagglehub.dataset_download(\"reubensuju/celeb-df-v2\")\n","\n","# print(\"Path to dataset files:\", path)"],"metadata":{"id":"WwcM1BRF2CTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import (\n","    Dense, Input, Layer, GlobalAveragePooling1D, LayerNormalization\n",")\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","\n","import tensorflow as tf\n","from keras.layers import Input, Dense, Lambda\n","from keras.models import Model"],"metadata":{"id":"ULJkYEq7bWcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ----------------------\n","# VisionTemporalTransformer (simplified, stable)\n","# ----------------------\n","class VisionTemporalTransformer(Layer):\n","    def __init__(self, patch_size=8, d_model=128, num_heads=4, spatial_layers=1, temporal_layers=1, **kwargs):\n","        super(VisionTemporalTransformer, self).__init__(**kwargs)\n","        self.patch_size = patch_size\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.spatial_layers = spatial_layers\n","        self.temporal_layers = temporal_layers\n","\n","        # basic projection for patches\n","        self.dense_projection = Dense(d_model)\n","        self.pos_emb = None\n","\n","        # build small stacks of MHA + FFN for spatial and temporal\n","        self.spatial_mhas = [tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads) for _ in range(spatial_layers)]\n","        self.spatial_norm1 = [LayerNormalization() for _ in range(spatial_layers)]\n","        self.spatial_ffn = [tf.keras.Sequential([Dense(d_model*4, activation='relu'), Dense(d_model)]) for _ in range(spatial_layers)]\n","        self.spatial_norm2 = [LayerNormalization() for _ in range(spatial_layers)]\n","\n","        self.temporal_mhas = [tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads) for _ in range(temporal_layers)]\n","        self.temporal_norm1 = [LayerNormalization() for _ in range(temporal_layers)]\n","        self.temporal_ffn = [tf.keras.Sequential([Dense(d_model*4, activation='relu'), Dense(d_model)]) for _ in range(temporal_layers)]\n","        self.temporal_norm2 = [LayerNormalization() for _ in range(temporal_layers)]\n","\n","    def build(self, input_shape):\n","        # input_shape: (batch, frames, H, W, C)\n","        _, frames, H, W, C = input_shape\n","        ph = H // self.patch_size\n","        pw = W // self.patch_size\n","        num_patches = ph * pw\n","        # positional embedding for patches\n","        self.pos_emb = self.add_weight(shape=(1, num_patches, self.d_model), initializer='random_normal', trainable=True, name='pos_emb')\n","        super(VisionTemporalTransformer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        # inputs shape: (batch, frames, H, W, C)\n","        batch = tf.shape(inputs)[0]\n","        frames = tf.shape(inputs)[1]\n","        H = tf.shape(inputs)[2]\n","        W = tf.shape(inputs)[3]\n","        C = tf.shape(inputs)[4]\n","\n","        # reshape to (batch*frames, H, W, C)\n","        reshaped = tf.reshape(inputs, (-1, H, W, C))\n","\n","        # extract patches\n","        patches = tf.image.extract_patches(\n","            images=reshaped,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1,1,1,1],\n","            padding='VALID'\n","        )\n","        # patches shape: (batch*frames, ph, pw, patch_dim)\n","        patch_dim = tf.shape(patches)[-1]\n","        num_patches = tf.shape(patches)[1] * tf.shape(patches)[2]\n","        patches = tf.reshape(patches, (-1, num_patches, patch_dim))\n","\n","        # linear projection\n","        x = self.dense_projection(patches) + self.pos_emb\n","\n","        # spatial transformer (per frame)\n","        for i in range(self.spatial_layers):\n","            attn = self.spatial_mhas[i](query=x, value=x, key=x)\n","            x = self.spatial_norm1[i](x + attn)\n","            ff = self.spatial_ffn[i](x)\n","            x = self.spatial_norm2[i](x + ff)\n","\n","        # reshape back to (batch, frames, num_patches, d_model)\n","        d_model = tf.shape(x)[-1]\n","        x = tf.reshape(x, (batch, frames, -1, d_model))\n","\n","        # temporal pooling across patches -> sequence of frame tokens\n","        x = tf.reduce_mean(x, axis=2)  # (batch, frames, d_model)\n","\n","        # temporal transformer\n","        for i in range(self.temporal_layers):\n","            attn = self.temporal_mhas[i](query=x, value=x, key=x)\n","            x = self.temporal_norm1[i](x + attn)\n","            ff = self.temporal_ffn[i](x)\n","            x = self.temporal_norm2[i](x + ff)\n","\n","        # pooled output per sample\n","        pooled = GlobalAveragePooling1D()(x)  # (batch, d_model)\n","        return pooled\n","\n","# ----------------------\n","# Consistency loss based on cosine similarity\n","# ----------------------\n","\n","def batch_consistency_loss(y_true, features):\n","    \"\"\"\n","    features: (batch, dim)\n","    compute for each sample i: loss_i = 1 - mean_j cosine_sim(f_i, f_j)\n","    return tensor shape (batch,)\n","    \"\"\"\n","    # flatten\n","    f = tf.reshape(features, (tf.shape(features)[0], -1))\n","    # L2 normalize\n","    f_norm = tf.math.l2_normalize(f, axis=1)\n","    sim_matrix = tf.matmul(f_norm, f_norm, transpose_b=True)  # (B, B)\n","    avg_sim = tf.reduce_mean(sim_matrix, axis=1)  # (B,)\n","    loss_per_sample = 1.0 - avg_sim\n","    return loss_per_sample\n","\n","# wrapper to fit signature (y_true, y_pred) -> loss\n","def consistency_loss_wrapper(y_true, y_pred):\n","    return batch_consistency_loss(y_true, y_pred)\n","\n","# ----------------------\n","# Build full model (two outputs)\n","# ----------------------\n","def build_lipinc_model(frame_shape=(8,64,144,3), residue_shape=(7,64,144,3), d_model=128):\n","\n","    frame_input = Input(shape=frame_shape, name='FrameInput')\n","    residue_input = Input(shape=residue_shape, name='ResidueInput')\n","\n","    # Vision Temporal Transformer\n","    vt = VisionTemporalTransformer(\n","        patch_size=8,\n","        d_model=d_model,\n","        num_heads=4,\n","        spatial_layers=1,\n","        temporal_layers=1\n","    )\n","\n","    frame_feat = vt(frame_input)      # (batch, d_model)\n","    residue_feat = vt(residue_input)  # (batch, d_model)\n","\n","    # -------------------------\n","    # FIXED: wrap tf.expand_dims\n","    # -------------------------\n","    expand1 = Lambda(lambda x: tf.expand_dims(x, axis=1))\n","    q = expand1(frame_feat)\n","    k = expand1(residue_feat)\n","    v = k\n","\n","    # MultiHeadAttention\n","    mha = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=d_model//4)\n","    attn_out = mha(query=q, value=v, key=k)  # (batch, 1, d_model)\n","\n","    # -------------------------\n","    # FIXED: wrap tf.squeeze\n","    # -------------------------\n","    squeeze = Lambda(lambda x: tf.squeeze(x, axis=1))\n","    attn_out = squeeze(attn_out)\n","\n","    # -------------------------\n","    # FIXED: wrap tf.concat\n","    # -------------------------\n","    concat = Lambda(lambda t: tf.concat(t, axis=1))\n","    fusion = concat([frame_feat, residue_feat, attn_out])\n","\n","    # Fusion MLP\n","    x = Dense(512, activation='relu')(fusion)\n","    x = Dense(256, activation='relu')(x)\n","\n","    # Classification head\n","    class_output = Dense(2, activation='softmax', name='class_output')(x)\n","\n","    # Features for consistency loss\n","    features_output = Dense(d_model, activation=None, name='features_output')(x)\n","\n","    model = Model(\n","        inputs=[frame_input, residue_input],\n","        outputs=[class_output, features_output],\n","        name='LIPINC_fixed'\n","    )\n","\n","    return model\n","\n","# ----------------------\n","# Synthetic data generator (for demo)\n","# ----------------------\n","\n","def generate_synthetic_dataset(num_samples=256, frame_shape=(8,64,144,3), residue_shape=(7,64,144,3), num_classes=2):\n","    X_frames = np.random.rand(num_samples, *frame_shape).astype(np.float32)\n","    X_residue = np.random.rand(num_samples, *residue_shape).astype(np.float32)\n","    y = np.random.randint(0, num_classes, size=(num_samples,))\n","    y_onehot = tf.keras.utils.to_categorical(y, num_classes=num_classes)\n","    return (X_frames, X_residue), y_onehot"],"metadata":{"id":"HPUZIzLKbXPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from glob import glob\n","import cv2\n","import numpy as np\n","import kagglehub\n","\n","# Download dataset\n","path = kagglehub.dataset_download(\"reubensuju/celeb-df-v2\")\n","print(\"Path to dataset files:\", path)\n","\n","DATASET_ROOT = path\n","\n","def load_video(filepath, frame_count=8):\n","    cap = cv2.VideoCapture(filepath)\n","    frames = []\n","    while len(frames) < frame_count:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (144, 64))\n","        frames.append(frame)\n","    cap.release()\n","    if len(frames) < frame_count:\n","        return None\n","    return np.array(frames) / 255.0\n","\n","def compute_residue(frames):\n","    residues = []\n","    for i in range(1, len(frames)):\n","        residues.append(frames[i] - frames[i-1])\n","    return np.array(residues)\n","\n","real_videos = glob(f\"{DATASET_ROOT}/Celeb-real/*.mp4\")\n","fake_videos = glob(f\"{DATASET_ROOT}/Celeb-synthesis/*.mp4\")\n","\n","X_frames, X_residues, Y = [], [], []\n","\n","for vid in real_videos[:200]:\n","    frames = load_video(vid)\n","    if frames is None:\n","        continue\n","    residues = compute_residue(frames)\n","    X_frames.append(frames)\n","    X_residues.append(residues)\n","    Y.append([1, 0])\n","\n","for vid in fake_videos[:200]:\n","    frames = load_video(vid)\n","    if frames is None:\n","        continue\n","    residues = compute_residue(frames)\n","    X_frames.append(frames)\n","    X_residues.append(residues)\n","    Y.append([0, 1])\n","\n","X_frames = np.array(X_frames)\n","X_residues = np.array(X_residues)\n","Y = np.array(Y)\n","\n","print(\"Dataset Loaded:\", X_frames.shape, X_residues.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"somiOjVubjbs","executionInfo":{"status":"ok","timestamp":1764536307260,"user_tz":-420,"elapsed":12981,"user":{"displayName":"Research Together","userId":"13238398503384701171"}},"outputId":"03161c68-8384-4ba4-d735-161ebd2d965d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Colab cache for faster access to the 'celeb-df-v2' dataset.\n","Path to dataset files: /kaggle/input/celeb-df-v2\n","Dataset Loaded: (400, 8, 64, 144, 3) (400, 7, 64, 144, 3)\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# 1) Split Train + Temp (Val + Test)\n","Xf_train, Xf_temp, Xr_train, Xr_temp, y_train, y_temp = train_test_split(\n","    X_frames, X_residues, Y,\n","    test_size=0.30,    # 30% untuk val+test\n","    random_state=42,\n","    shuffle=True,\n","    stratify=Y\n",")\n","\n","# 2) Split Temp menjadi Validation + Test\n","Xf_val, Xf_test, Xr_val, Xr_test, y_val, y_test = train_test_split(\n","    Xf_temp, Xr_temp, y_temp,\n","    test_size=0.50,   # 15% val + 15% test\n","    random_state=42,\n","    shuffle=True,\n","    stratify=y_temp\n",")\n","\n","print(\"Train :\", Xf_train.shape, Xr_train.shape, y_train.shape)\n","print(\"Val   :\", Xf_val.shape,   Xr_val.shape,   y_val.shape)\n","print(\"Test  :\", Xf_test.shape,  Xr_test.shape,  y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dJ3ZneB4dYAg","executionInfo":{"status":"ok","timestamp":1764536315451,"user_tz":-420,"elapsed":465,"user":{"displayName":"Research Together","userId":"13238398503384701171"}},"outputId":"88aaa005-5a01-45d5-bcc1-b03c0128fa7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train : (280, 8, 64, 144, 3) (280, 7, 64, 144, 3) (280, 2)\n","Val   : (60, 8, 64, 144, 3) (60, 7, 64, 144, 3) (60, 2)\n","Test  : (60, 8, 64, 144, 3) (60, 7, 64, 144, 3) (60, 2)\n"]}]},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# 1. Build model\n","# ------------------------------------------------------------\n","model = build_lipinc_model()\n","model.summary()\n","\n","# Optimizer\n","opt = Adam(learning_rate=1e-3)\n","\n","# Compile model with 2 losses\n","model.compile(\n","    optimizer=opt,\n","    loss={\n","        'class_output': 'categorical_crossentropy',\n","        'features_output': consistency_loss_wrapper\n","    },\n","    loss_weights={\n","        'class_output': 1.0,\n","        'features_output': 5.0\n","    },\n","    metrics={'class_output': 'accuracy'}\n",")\n","\n","\n","# ------------------------------------------------------------\n","# 3. Buat dummy target untuk output kedua (features_output)\n","# ------------------------------------------------------------\n","dummy_train = np.zeros((len(y_train), 128), dtype=np.float32)\n","dummy_val   = np.zeros((len(y_val),   128), dtype=np.float32)\n","dummy_test  = np.zeros((len(y_test),  128), dtype=np.float32)\n","\n","\n","# ------------------------------------------------------------\n","# 4. TRAIN MODEL\n","# ------------------------------------------------------------\n","history = model.fit(\n","    x=[Xf_train, Xr_train],\n","    y={\n","        'class_output': y_train,\n","        'features_output': dummy_train\n","    },\n","    validation_data=(\n","        [Xf_val, Xr_val],\n","        {\n","            'class_output': y_val,\n","            'features_output': dummy_val\n","        }\n","    ),\n","    epochs=50,\n","    batch_size=16,\n","    shuffle=True\n",")\n","\n","\n","# ------------------------------------------------------------\n","# 5. EVALUATE MODEL\n","# ------------------------------------------------------------\n","results = model.evaluate(\n","    x=[Xf_test, Xr_test],\n","    y={\n","        'class_output': y_test,\n","        'features_output': dummy_test\n","    }\n",")\n","\n","print(\"\\nTest Results:\")\n","print(results)\n","\n","\n","# ------------------------------------------------------------\n","# 6. SAVE MODEL\n","# ------------------------------------------------------------\n","model.save('lipinc_celebdf2_trained.h5')\n","print(\"Model saved to lipinc_celebdf2_trained.h5\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GntjwaW1bnKF","executionInfo":{"status":"ok","timestamp":1764536421346,"user_tz":-420,"elapsed":102325,"user":{"displayName":"Research Together","userId":"13238398503384701171"}},"outputId":"49e327ab-2de0-45dd-b41e-f00bd4f6a403"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"LIPINC_fixed\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LIPINC_fixed\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ FrameInput          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ ResidueInput        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ vision_temporal_tr… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m439,680\u001b[0m │ FrameInput[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n","│ (\u001b[38;5;33mVisionTemporalTra…\u001b[0m │                   │            │ ResidueInput[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ vision_temporal_… │\n","│                     │                   │            │ vision_temporal_… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │     \u001b[38;5;34m66,048\u001b[0m │ lambda[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n","│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n","│                     │                   │            │ lambda[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ vision_temporal_… │\n","│                     │                   │            │ vision_temporal_… │\n","│                     │                   │            │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m197,120\u001b[0m │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ class_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m514\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ features_output     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ FrameInput          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ ResidueInput        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ vision_temporal_tr… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">439,680</span> │ FrameInput[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">VisionTemporalTra…</span> │                   │            │ ResidueInput[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vision_temporal_… │\n","│                     │                   │            │ vision_temporal_… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n","│                     │                   │            │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vision_temporal_… │\n","│                     │                   │            │ vision_temporal_… │\n","│                     │                   │            │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ class_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ features_output     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m867,586\u001b[0m (3.31 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">867,586</span> (3.31 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m867,586\u001b[0m (3.31 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">867,586</span> (3.31 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - class_output_accuracy: 0.4413 - class_output_loss: 1.1164 - features_output_loss: 0.0043 - loss: 1.1384 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6940 - val_features_output_loss: 8.1603e-05 - val_loss: 0.6950\n","Epoch 2/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - class_output_accuracy: 0.4504 - class_output_loss: 0.7204 - features_output_loss: 4.2142e-05 - loss: 0.7205 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.7576 - val_features_output_loss: 2.7440e-05 - val_loss: 0.7617\n","Epoch 3/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.5093 - class_output_loss: 0.7122 - features_output_loss: 1.4728e-05 - loss: 0.7121 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6955 - val_features_output_loss: 2.1712e-05 - val_loss: 0.6949\n","Epoch 4/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.4157 - class_output_loss: 0.7073 - features_output_loss: 2.1220e-05 - loss: 0.7073 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.7552 - val_features_output_loss: 2.0573e-05 - val_loss: 0.7517\n","Epoch 5/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.4980 - class_output_loss: 0.7290 - features_output_loss: 2.2411e-05 - loss: 0.7291 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6977 - val_features_output_loss: 3.0759e-05 - val_loss: 0.6969\n","Epoch 6/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.4618 - class_output_loss: 0.7108 - features_output_loss: 2.1135e-05 - loss: 0.7109 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.7044 - val_features_output_loss: 3.0053e-05 - val_loss: 0.7063\n","Epoch 7/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - class_output_accuracy: 0.5460 - class_output_loss: 0.6983 - features_output_loss: 1.5349e-05 - loss: 0.6987 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.7843 - val_features_output_loss: 2.2468e-05 - val_loss: 0.7890\n","Epoch 8/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - class_output_accuracy: 0.5234 - class_output_loss: 0.7285 - features_output_loss: 1.1941e-05 - loss: 0.7286 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6929 - val_features_output_loss: 2.2687e-05 - val_loss: 0.6932\n","Epoch 9/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - class_output_accuracy: 0.4590 - class_output_loss: 0.6970 - features_output_loss: 1.4974e-05 - loss: 0.6973 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6988 - val_features_output_loss: 1.4653e-05 - val_loss: 0.6979\n","Epoch 10/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.5167 - class_output_loss: 0.6928 - features_output_loss: 1.0556e-05 - loss: 0.6929 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6937 - val_features_output_loss: 1.1726e-05 - val_loss: 0.6943\n","Epoch 11/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.5244 - class_output_loss: 0.7039 - features_output_loss: 1.0851e-05 - loss: 0.7038 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6962 - val_features_output_loss: 1.4910e-05 - val_loss: 0.6956\n","Epoch 12/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.4914 - class_output_loss: 0.6939 - features_output_loss: 1.3654e-05 - loss: 0.6939 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6937 - val_features_output_loss: 1.2964e-05 - val_loss: 0.6935\n","Epoch 13/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.4630 - class_output_loss: 0.6984 - features_output_loss: 9.1295e-06 - loss: 0.6984 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6935 - val_features_output_loss: 1.8837e-05 - val_loss: 0.6934\n","Epoch 14/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - class_output_accuracy: 0.5505 - class_output_loss: 0.6935 - features_output_loss: 9.0457e-06 - loss: 0.6935 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6945 - val_features_output_loss: 2.7386e-05 - val_loss: 0.6953\n","Epoch 15/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.4764 - class_output_loss: 0.6989 - features_output_loss: 1.0776e-05 - loss: 0.6990 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6930 - val_features_output_loss: 2.7569e-05 - val_loss: 0.6934\n","Epoch 16/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.5367 - class_output_loss: 0.6942 - features_output_loss: 1.3241e-05 - loss: 0.6941 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6944 - val_features_output_loss: 1.4211e-05 - val_loss: 0.6951\n","Epoch 17/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.4764 - class_output_loss: 0.7008 - features_output_loss: 1.1385e-05 - loss: 0.7009 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6944 - val_features_output_loss: 1.5861e-05 - val_loss: 0.6952\n","Epoch 18/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.4563 - class_output_loss: 0.6991 - features_output_loss: 5.7380e-06 - loss: 0.6992 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6973 - val_features_output_loss: 7.6471e-06 - val_loss: 0.6984\n","Epoch 19/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.5097 - class_output_loss: 0.6970 - features_output_loss: 4.7026e-06 - loss: 0.6970 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6938 - val_features_output_loss: 2.0349e-05 - val_loss: 0.6937\n","Epoch 20/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - class_output_accuracy: 0.4980 - class_output_loss: 0.6954 - features_output_loss: 9.4722e-06 - loss: 0.6955 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6935 - val_features_output_loss: 2.5212e-05 - val_loss: 0.6935\n","Epoch 21/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - class_output_accuracy: 0.5056 - class_output_loss: 0.7002 - features_output_loss: 1.9387e-05 - loss: 0.7003 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6933 - val_features_output_loss: 2.7651e-05 - val_loss: 0.6934\n","Epoch 22/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.4673 - class_output_loss: 0.6958 - features_output_loss: 8.3553e-06 - loss: 0.6959 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 1.3172e-05 - val_loss: 0.6933\n","Epoch 23/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.5011 - class_output_loss: 0.6941 - features_output_loss: 5.3369e-06 - loss: 0.6942 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6937 - val_features_output_loss: 6.2473e-06 - val_loss: 0.6935\n","Epoch 24/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - class_output_accuracy: 0.4525 - class_output_loss: 0.6981 - features_output_loss: 2.9443e-06 - loss: 0.6981 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6935 - val_features_output_loss: 5.7978e-06 - val_loss: 0.6939\n","Epoch 25/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.5224 - class_output_loss: 0.6929 - features_output_loss: 2.8756e-06 - loss: 0.6929 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6931 - val_features_output_loss: 1.2362e-06 - val_loss: 0.6932\n","Epoch 26/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.4734 - class_output_loss: 0.6938 - features_output_loss: 2.8333e-06 - loss: 0.6938 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6933 - val_features_output_loss: 2.3793e-05 - val_loss: 0.6933\n","Epoch 27/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - class_output_accuracy: 0.4773 - class_output_loss: 0.6934 - features_output_loss: 1.9929e-05 - loss: 0.6935 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6931 - val_features_output_loss: -1.1921e-07 - val_loss: 0.6932\n","Epoch 28/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.5026 - class_output_loss: 0.6932 - features_output_loss: 1.5373e-06 - loss: 0.6932 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6931 - val_features_output_loss: 9.3514e-06 - val_loss: 0.6932\n","Epoch 29/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.4873 - class_output_loss: 0.6934 - features_output_loss: 1.0602e-06 - loss: 0.6934 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6931 - val_features_output_loss: 7.4506e-08 - val_loss: 0.6932\n","Epoch 30/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.4689 - class_output_loss: 0.6933 - features_output_loss: 3.7831e-08 - loss: 0.6933 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 1.6205e-07 - val_loss: 0.6932\n","Epoch 31/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - class_output_accuracy: 0.4740 - class_output_loss: 0.6933 - features_output_loss: 1.2533e-06 - loss: 0.6933 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6931 - val_features_output_loss: -2.0862e-07 - val_loss: 0.6932\n","Epoch 32/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - class_output_accuracy: 0.4765 - class_output_loss: 0.6935 - features_output_loss: -3.4601e-08 - loss: 0.6935 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6933 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6932\n","Epoch 33/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - class_output_accuracy: 0.5001 - class_output_loss: 0.6932 - features_output_loss: 5.1810e-08 - loss: 0.6932 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: -1.1921e-07 - val_loss: 0.6932\n","Epoch 34/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.4872 - class_output_loss: 0.6934 - features_output_loss: 1.1547e-08 - loss: 0.6934 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6931\n","Epoch 35/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.5149 - class_output_loss: 0.6932 - features_output_loss: 2.5717e-08 - loss: 0.6932 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6932\n","Epoch 36/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - class_output_accuracy: 0.4957 - class_output_loss: 0.6932 - features_output_loss: 2.2925e-08 - loss: 0.6932 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 1.0431e-07 - val_loss: 0.6932\n","Epoch 37/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - class_output_accuracy: 0.5197 - class_output_loss: 0.6931 - features_output_loss: 4.1044e-09 - loss: 0.6931 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 5.9605e-08 - val_loss: 0.6932\n","Epoch 38/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.4495 - class_output_loss: 0.6934 - features_output_loss: 6.4963e-09 - loss: 0.6934 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6931 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6932\n","Epoch 39/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.4949 - class_output_loss: 0.6932 - features_output_loss: -8.7361e-09 - loss: 0.6932 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6931\n","Epoch 40/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.5007 - class_output_loss: 0.6932 - features_output_loss: -1.6231e-09 - loss: 0.6932 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6931 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6932\n","Epoch 41/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.5030 - class_output_loss: 0.6931 - features_output_loss: -1.3100e-08 - loss: 0.6931 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6931 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6931\n","Epoch 42/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.5140 - class_output_loss: 0.6933 - features_output_loss: -3.7645e-09 - loss: 0.6933 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6931\n","Epoch 43/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.4316 - class_output_loss: 0.6933 - features_output_loss: 2.0163e-08 - loss: 0.6933 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6931 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6931\n","Epoch 44/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - class_output_accuracy: 0.5315 - class_output_loss: 0.6931 - features_output_loss: -1.4051e-08 - loss: 0.6931 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: -1.1921e-07 - val_loss: 0.6931\n","Epoch 45/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - class_output_accuracy: 0.5269 - class_output_loss: 0.6931 - features_output_loss: 3.1998e-09 - loss: 0.6931 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: -2.0862e-07 - val_loss: 0.6932\n","Epoch 46/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.4515 - class_output_loss: 0.6936 - features_output_loss: -2.2976e-08 - loss: 0.6936 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6931 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6931\n","Epoch 47/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - class_output_accuracy: 0.5099 - class_output_loss: 0.6931 - features_output_loss: -6.9195e-09 - loss: 0.6931 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 5.9605e-08 - val_loss: 0.6932\n","Epoch 48/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.5007 - class_output_loss: 0.6932 - features_output_loss: 5.1410e-08 - loss: 0.6932 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 1.0431e-07 - val_loss: 0.6931\n","Epoch 49/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - class_output_accuracy: 0.4008 - class_output_loss: 0.6934 - features_output_loss: 4.6758e-08 - loss: 0.6934 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 0.0000e+00 - val_loss: 0.6932\n","Epoch 50/50\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - class_output_accuracy: 0.5059 - class_output_loss: 0.6932 - features_output_loss: -2.6212e-10 - loss: 0.6932 - val_class_output_accuracy: 0.5000 - val_class_output_loss: 0.6932 - val_features_output_loss: 1.0431e-07 - val_loss: 0.6932\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - class_output_accuracy: 0.5104 - class_output_loss: 0.6931 - features_output_loss: 7.9473e-08 - loss: 0.6931\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Results:\n","[0.6931575536727905, 0.6931772232055664, 5.960464477539063e-08, 0.5]\n","Model saved to lipinc_celebdf2_trained.h5\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","# ============================================================\n","# FUNGSI PREPROCESSING VIDEO\n","# ============================================================\n","\n","def load_video_for_test(video_path, frame_count=8, target_size=(144, 64)):\n","    \"\"\"\n","    Load video dan ekstrak frame untuk testing\n","\n","    Args:\n","        video_path: Path ke file video\n","        frame_count: Jumlah frame yang akan diambil (default: 8)\n","        target_size: (width, height) untuk resize frame (default: 144, 64)\n","\n","    Returns:\n","        frames: numpy array shape (frame_count, height, width, 3)\n","        residues: numpy array shape (frame_count-1, height, width, 3)\n","    \"\"\"\n","    cap = cv2.VideoCapture(video_path)\n","\n","    if not cap.isOpened():\n","        raise ValueError(f\"❌ Cannot open video: {video_path}\")\n","\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    duration = total_frames / fps if fps > 0 else 0\n","\n","    print(f\"\\n📹 Video Information:\")\n","    print(f\"   File: {video_path}\")\n","    print(f\"   Total frames: {total_frames}\")\n","    print(f\"   FPS: {fps:.2f}\")\n","    print(f\"   Duration: {duration:.2f} seconds\")\n","    print(f\"   Resolution: {int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}\")\n","\n","    frames = []\n","    # Sample frames evenly across the video (avoid last frame to prevent read errors)\n","    frame_indices = np.linspace(0, max(0, total_frames - 2), frame_count, dtype=int)\n","\n","    print(f\"\\n🎬 Extracting {frame_count} frames at indices: {frame_indices}\")\n","\n","    for idx in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","\n","        if not ret:\n","            print(f\"⚠️  Warning: Cannot read frame at index {idx}, trying sequential read...\")\n","            # Fallback: try reading sequentially\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, max(0, idx - 1))\n","            ret, frame = cap.read()\n","            if not ret:\n","                print(f\"⚠️  Skipping frame at index {idx}\")\n","                continue\n","\n","        # Resize frame to model input size\n","        frame = cv2.resize(frame, target_size)\n","        # Convert BGR to RGB\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        frames.append(frame)\n","\n","    cap.release()\n","\n","    # If we got less frames than needed, pad with duplicate of last frame\n","    if len(frames) < frame_count:\n","        print(f\"⚠️  Only got {len(frames)} frames, padding to {frame_count} frames...\")\n","        while len(frames) < frame_count:\n","            frames.append(frames[-1].copy())  # duplicate last frame\n","\n","    # Normalize frames to [0, 1]\n","    frames = np.array(frames, dtype=np.float32) / 255.0\n","\n","    # Compute residues (frame differences)\n","    residues = []\n","    for i in range(1, len(frames)):\n","        residue = frames[i] - frames[i-1]\n","        residues.append(residue)\n","\n","    residues = np.array(residues, dtype=np.float32)\n","\n","    print(f\"✅ Preprocessing complete!\")\n","    print(f\"   Frames shape: {frames.shape}\")\n","    print(f\"   Residues shape: {residues.shape}\")\n","\n","    return frames, residues\n","\n","\n","# ============================================================\n","# TEST VIDEO\n","# ============================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\" \" * 20 + \"TESTING VIDEO: video.mp4\")\n","print(\"=\"*70)\n","\n","# Load and preprocess video\n","video_path = \"video.mp4\"\n","\n","try:\n","    frames, residues = load_video_for_test(video_path)\n","\n","    # Add batch dimension\n","    frames_batch = np.expand_dims(frames, axis=0)      # (1, 8, 64, 144, 3)\n","    residues_batch = np.expand_dims(residues, axis=0)  # (1, 7, 64, 144, 3)\n","\n","    print(f\"\\n🔮 Running prediction...\")\n","\n","    # Predict using the trained model\n","    predictions = model.predict([frames_batch, residues_batch], verbose=0)\n","    class_output = predictions[0]  # (1, 2) - [prob_real, prob_fake]\n","    features_output = predictions[1]  # (1, 128)\n","\n","    # Extract probabilities\n","    prob_real = class_output[0, 0]\n","    prob_fake = class_output[0, 1]\n","\n","    # Determine label (threshold = 0.5)\n","    threshold = 0.5\n","    predicted_label = \"FAKE\" if prob_fake > threshold else \"REAL\"\n","    confidence = max(prob_real, prob_fake) * 100\n","\n","    # Print results\n","    print(\"\\n\" + \"=\"*70)\n","    print(\" \" * 25 + \"🎯 PREDICTION RESULTS\")\n","    print(\"=\"*70)\n","    print(f\"\\n{'Metric':<30s} {'Value':>15s}\")\n","    print(\"-\"*70)\n","    print(f\"{'Real Probability':<30s} {prob_real:>10.4f} ({prob_real*100:>6.2f}%)\")\n","    print(f\"{'Fake Probability':<30s} {prob_fake:>10.4f} ({prob_fake*100:>6.2f}%)\")\n","    print(\"-\"*70)\n","    print(f\"{'Predicted Label':<30s} {predicted_label:>15s}\")\n","    print(f\"{'Confidence':<30s} {confidence:>14.2f}%\")\n","    print(f\"{'Threshold Used':<30s} {threshold:>15.2f}\")\n","    print(\"=\"*70)\n","\n","    # Visual indicator\n","    if predicted_label == \"FAKE\":\n","        print(\"\\n⚠️  WARNING: This video is predicted to be FAKE/DEEPFAKE!\")\n","    else:\n","        print(\"\\n✅ This video is predicted to be REAL/AUTHENTIC.\")\n","\n","    print(\"\\n\" + \"=\"*70 + \"\\n\")\n","\n","    # Store results for later analysis\n","    video_result = {\n","        'video_path': video_path,\n","        'predicted_label': predicted_label,\n","        'prob_real': float(prob_real),\n","        'prob_fake': float(prob_fake),\n","        'confidence': float(confidence),\n","        'threshold': threshold\n","    }\n","\n","    print(\"📊 Result Summary:\")\n","    for key, value in video_result.items():\n","        print(f\"   {key}: {value}\")\n","\n","except FileNotFoundError:\n","    print(f\"\\n❌ Error: Video file '{video_path}' not found!\")\n","    print(\"   Please make sure the video file exists in the current directory.\")\n","\n","except Exception as e:\n","    print(f\"\\n❌ Error occurred while processing video:\")\n","    print(f\"   {type(e).__name__}: {str(e)}\")\n","    import traceback\n","    traceback.print_exc()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zaGHyWdmdzEU","executionInfo":{"status":"ok","timestamp":1764489254668,"user_tz":-420,"elapsed":6243,"user":{"displayName":"Ikhlasul Amal","userId":"06032023784132959017"}},"outputId":"cda703f0-1e87-4fb4-fef7-58dbfc47bf69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","                    TESTING VIDEO: video.mp4\n","======================================================================\n","\n","📹 Video Information:\n","   File: video.mp4\n","   Total frames: 332\n","   FPS: 30.01\n","   Duration: 11.06 seconds\n","   Resolution: 576x1024\n","\n","🎬 Extracting 8 frames at indices: [  0  47  94 141 188 235 282 330]\n","✅ Preprocessing complete!\n","   Frames shape: (8, 64, 144, 3)\n","   Residues shape: (7, 64, 144, 3)\n","\n","🔮 Running prediction...\n","\n","======================================================================\n","                         🎯 PREDICTION RESULTS\n","======================================================================\n","\n","Metric                                   Value\n","----------------------------------------------------------------------\n","Real Probability                   0.4993 ( 49.93%)\n","Fake Probability                   0.5007 ( 50.07%)\n","----------------------------------------------------------------------\n","Predicted Label                           FAKE\n","Confidence                              50.07%\n","Threshold Used                            0.50\n","======================================================================\n","\n","⚠️  WARNING: This video is predicted to be FAKE/DEEPFAKE!\n","\n","======================================================================\n","\n","📊 Result Summary:\n","   video_path: video.mp4\n","   predicted_label: FAKE\n","   prob_real: 0.4992809593677521\n","   prob_fake: 0.5007190108299255\n","   confidence: 50.0718994140625\n","   threshold: 0.5\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# RE-SAVE MODEL TO .keras FORMAT (Run this after training)\n","# ============================================================\n","\n","print(\"Re-saving model to .keras format...\")\n","\n","# Load the .h5 model\n","from tensorflow.keras.models import load_model\n","\n","# Define custom objects (copy from training code)\n","custom_objects = {\n","    'VisionTemporalTransformer': VisionTemporalTransformer,\n","    'consistency_loss_wrapper': consistency_loss_wrapper\n","}\n","\n","# Load H5 model\n","model_h5 = load_model('lipinc_celebdf2_trained.h5',\n","                       custom_objects=custom_objects,\n","                       compile=False)\n","\n","# Save in .keras format\n","model_h5.save('lipinc_celebdf2_trained.keras')\n","print(\"✅ Model saved to lipinc_celebdf2_trained.keras\")\n","\n","# Now you can load it without issues:\n","# model = load_model('lipinc_celebdf2_trained.keras', custom_objects=custom_objects)"],"metadata":{"id":"IgIbLVu0gMWW","colab":{"base_uri":"https://localhost:8080/","height":530},"executionInfo":{"status":"error","timestamp":1764488870940,"user_tz":-420,"elapsed":961,"user":{"displayName":"Ikhlasul Amal","userId":"06032023784132959017"}},"outputId":"ce062350-8953-4f68-fb91-4e2ed7775900"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Re-saving model to .keras format...\n"]},{"output_type":"error","ename":"NotImplementedError","evalue":"Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, 128), dtype=float32, sparse=False, ragged=False, name=keras_tensor_141>',)\n  • kwargs={'mask': 'None'}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2652703103.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Load H5 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m model_h5 = load_model('lipinc_celebdf2_trained.h5', \n\u001b[0m\u001b[1;32m     18\u001b[0m                        \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                        compile=False)\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msaving_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_option_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_legacy_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             model = saving_utils.model_from_config(\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     return serialization.deserialize_keras_object(\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODULE_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"custom_objects\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    496\u001b[0m                     \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                     custom_objects={\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    652\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0mnode_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_data_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                         \u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                     \u001b[0;31m# If the node does not have all inbound layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# Call layer on its inputs, thus creating the node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# and building the layer if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 raise NotImplementedError(\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0;34m\"We could not automatically infer the shape of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;34m\"the Lambda's output. Please specify the `output_shape` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, 128), dtype=float32, sparse=False, ragged=False, name=keras_tensor_141>',)\n  • kwargs={'mask': 'None'}"]}]}]}